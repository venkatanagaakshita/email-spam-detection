{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam Detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ8m1cBSmPdh"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import email"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c1nMnDHqTgT",
        "outputId": "08b21b5a-c867-459d-b56e-b1f89e12c639"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhJzYRNpnw67"
      },
      "source": [
        "path = '/content/gdrive/My Drive/data/'"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Gi5sZ7qkwc"
      },
      "source": [
        "easy_ham_paths = glob.glob(path+'easy_ham/*')\n",
        "spam_paths = glob.glob(path+'spam/*')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvCi2uogrCxT",
        "outputId": "3a413247-a5de-4e36-8648-6ed0aab7c302"
      },
      "source": [
        "ham_sample = np.array([train_test_split(easy_ham_paths, test_size = 0.3)])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_SpROuFuTVS"
      },
      "source": [
        "ham_train = np.array([])\n",
        "ham_test = np.array([])\n",
        "for o in ham_sample:\n",
        "    ham_train = np.concatenate((ham_train,o[0]),axis=0)\n",
        "    ham_test = np.concatenate((ham_test,o[1]),axis=0)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBdub0VVuXPS",
        "outputId": "613fe3fb-f052-42ee-ae5a-c643b23a6b21"
      },
      "source": [
        "spam_sample = np.array([train_test_split(spam_paths, test_size = 0.3)])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI9s_tNNukvK"
      },
      "source": [
        "spam_train = np.array([])\n",
        "spam_test = np.array([])\n",
        "for o in spam_sample:\n",
        "    spam_train = np.concatenate((spam_train,o[0]),axis=0)\n",
        "    spam_test = np.concatenate((spam_test,o[1]),axis=0)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3s1D6wtup4v"
      },
      "source": [
        "ham_train_label = [0]*ham_train.shape[0]\n",
        "spam_train_label = [1]*spam_train.shape[0]\n",
        "x_train = np.concatenate((ham_train,spam_train))\n",
        "y_train = np.concatenate((ham_train_label,spam_train_label))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_UpZISCurgn"
      },
      "source": [
        "ham_test_label = [0]*ham_test.shape[0]\n",
        "spam_test_label = [1]*spam_test.shape[0]\n",
        "x_test = np.concatenate((ham_test,spam_test))\n",
        "y_test = np.concatenate((ham_test_label,spam_test_label))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbddw-fbuv-4"
      },
      "source": [
        "train_shuffle_index = np.random.permutation(np.arange(0,x_train.shape[0]))\n",
        "test_shuffle_index = np.random.permutation(np.arange(0,x_test.shape[0]))\n",
        "x_train = x_train[train_shuffle_index]\n",
        "y_train = y_train[train_shuffle_index]\n",
        "x_test = x_test[test_shuffle_index]\n",
        "y_test = y_test[test_shuffle_index]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMLsqMqDu-VV"
      },
      "source": [
        "def remove_null(datas,labels):\n",
        "    not_null_idx = [i for i,o in enumerate(datas) if o is not None]\n",
        "    return np.array(datas)[not_null_idx],np.array(labels)[not_null_idx]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWgEZmnivBMb"
      },
      "source": [
        "x_train,y_train = remove_null(x_train,y_train)\n",
        "x_test,y_test = remove_null(x_test,y_test)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b-EXQTLviE3"
      },
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Z-zsI7viK7"
      },
      "source": [
        "def replace_newline(word):\n",
        "    return word.replace('\\n','')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzg4m-xAviRN"
      },
      "source": [
        "def remove_number(word):\n",
        "    result = re.sub(r'\\d+', '', word)\n",
        "    return result"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUYvxkOGvvJA"
      },
      "source": [
        "def to_lower(word):\n",
        "    result = word.lower()\n",
        "    return result"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiQu9m-OwGQW"
      },
      "source": [
        "def remove_punctuation(word):\n",
        "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
        "    return result"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s863kFPxwLsZ"
      },
      "source": [
        "def clean_up_pipeline(sentence):\n",
        "    cleaning_utils = [\n",
        "                      replace_newline,\n",
        "                      to_lower,\n",
        "                      remove_number,\n",
        "                      remove_punctuation]\n",
        "    for o in cleaning_utils:\n",
        "        sentence = o(sentence)\n",
        "    return sentence"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LwyjyHAwXfX"
      },
      "source": [
        "x_train = [clean_up_pipeline(o) for o in x_train]\n",
        "x_test = [clean_up_pipeline(o) for o in x_test]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "819UWmjkwXnT",
        "outputId": "fe190960-9dde-4b95-8fdd-79199097e487"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "from nltk.stem import PorterStemmer\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4v8x1VywXvS"
      },
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13H-SzE3w5sc",
        "outputId": "9018e294-2432-4be9-9956-430bfb3b4cb5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "x_train = [word_tokenize(o) for o in x_train]\n",
        "x_test = [word_tokenize(o) for o in x_test]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcQIT-k1w5vt"
      },
      "source": [
        "def remove_stop_words(words):\n",
        "    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
        "    return result"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzO5qXuUw5y0"
      },
      "source": [
        "def word_stemmer(words):\n",
        "    return [stemmer.stem(o) for o in words]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sumORIMw514"
      },
      "source": [
        "def clean_token_pipeline(words):\n",
        "    cleaning_utils = [remove_stop_words,word_stemmer]\n",
        "    for o in cleaning_utils:\n",
        "        words = o(words)\n",
        "    return words"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4rvuoxDxdlf"
      },
      "source": [
        "x_train = [clean_token_pipeline(o) for o in x_train]\n",
        "x_test = [clean_token_pipeline(o) for o in x_test]"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Fz5Bn-xdpZ"
      },
      "source": [
        "x_train = [\" \".join(o) for o in x_train]\n",
        "x_test = [\" \".join(o) for o in x_test]"
      ],
      "execution_count": 64,
      "outputs": []
    }
  ]
}